# 口语语言模型（SLMs）

## 1. 发展状况

分类：

-  “纯” 语音语言模型（**处理语音token**）
  - 仅使用无标注的token语音数据训练，以 “下一个令牌预测” 为目标（与文本 LLMs 的**预训练**阶段类似）
- 语音编码器与文本语言模型相结合的模型（输入输出同时包含**文本**）
  - **语音 + 文本 SLM**：使用配对的（**语音、文本转录**）数据训练，也是**预训练模型＋post training**
  - **语音感知文本 LM**：既保留文本 LLMs 的**IF**特性，又能对输入音频片段进行推理 → 响应输入语音和文本指令的期望输出文本的分布。**微调 LLMs＋post training**

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119131734806.png" alt="image-20251119131734806" style="zoom:50%;" /> 

常见语音任务： **SOTA** 处理方法 → 将预训练的自监督**编码器**模型与任务特定的预测 “头部”（**head**）相结合。 

- 自动语音识别（ASR）
- 语音翻译（ST）
- 口语语言理解（SLU）
- 说话人识别（SID）



## 2. 模型架构

通用架构与LLM不同在于：

- 语音解码器替换普通解码器
- 输入输出根据需要选择语音与文本

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119134053265.png" alt="image-20251119134053265" style="zoom: 50%;" /> 



## 3. 模型组件

### 3.1 Speech Encoder

语音是连续的波形，不像文字可以根据byte做token化，其语言信息与其他声学特征也**相互融合、无法分离**。因此，核心任务是从连续波形中提取有意义的表示。（与NLP的思路类似）

总览：

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119144746141.png" alt="image-20251119144746141" style="zoom:50%;" /> 

#### 3.1.1 **语音表示模型**

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119145453620.png" alt="image-20251119145453620" style="zoom:50%;" /> 

任务：将语音信号转换为连续的特征向量，然后再转化为离散的token（也可以不转）

- 纯语音模型和语音 + 文本语言模型倾向于使用**离散**语音令牌，而语音感知文本语言模型则倾向于使用**连续**表示
- 连续的特征可能包括：
  - 传统频谱图特征（声音的频率、强度等，可以做**语音识别**）
  - 基于自监督学习（SSL）的语音编码器的隐藏表示（语言逻辑，可以做**语言感知**）
  - 有监督预训练模型的隐藏表示（与文字匹配，做**翻译**）
  - 神经音频编解码器模型的隐藏表示（提取关键信息，做文本**转语音**）

#### 3.1.2 语音token转换

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119151628072.png" alt="image-20251119151628072" style="zoom:50%;" /> 

语音令牌分为两种：“语音学令牌（phonetic tokens）” 和 “音频编解码器令牌（audio codec tokens）”

- 语音学令牌：

  - 获取方式：对**自监督**语音模型表示进行量化（主要） / **预训练**语音识别模型的有监督编码器中获取

  - 特点：

    - 直接从**原始语音**中学习到令牌

    - 能够捕捉语言信息，这类令牌类似“**伪文本**”，使模型能以类似文本语言模型的方式工作，并生成语音学层面连贯的语音。

  - 注意点：

    - 自监督学习语音模型的不同层通常会编码不同类型的信息，通常选择**富含语音学信息**的层的表示。
    - **聚类数量**是关键超参数。聚类数量过少可能导致细粒度语音学信息丢失，而数量过多则可能引入不必要的特征

- 音频编解码器令牌：捕捉更细致的声学特征

  - 组件：
    - 将原始音频转换为帧级特征的encoder
    - 将特征转换为离散令牌的向量量化（VQ）模块（常用方法是残差向量量化**RVQ**，为每个时间步生成多个离散令牌，对应**多个粒度**级别 —— 其中第一级编码大部分信息，其余级别则编码残差信息）
    - 从令牌中重建音频的decoder
  - 架构：主干网络以**卷积**架构或**Transformer**为主

- 对比：

  |                  | 语音学令牌（Phonetic Tokens）                                | 音频编解码器令牌（Audio Codec Tokens）                       |
  | ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | **核心特点**     | 弱化说话人特定信息，聚焦语音的**语言内容**（如音素、句法逻辑），可视为 “伪文本” | 保留说话人身份、声学细节（如音色、韵律），源自神经编解码器，**还原音频特征** |
  | **适用任务场景** | **理解**类任务：区分词与非词、判断句法正确性                  **内容导向**任务：语音到语音翻译 | 声学细节敏感任务：个性化语音生成、语音克隆                         高保真需求任务：音频重建、高质量语音合成 |

#### 3.1.3 时间压缩

由于语音信号的**帧率较高**，量化语音特征序列的长度通常很长，通常会缩短序列长度。

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119155312885.png" alt="image-20251119155312885" style="zoom:50%;" /> 

压缩方法：

- 去重（deduplication）：把连续相同的令牌合并为单个令牌，但会丢失单个令牌的时长信息

- 字节对编码（BPE）

  

### 3.2  Speech Modality Adapter

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119184855162.png" alt="image-20251119184855162" style="zoom:50%;" /> 

**匹配**语音编码器的输出与序列模型的预期输入（语音编码器和序列模型通常是分开训练的）

- 特点：

  - 通常会在**下游任务**上进行训练（对于语音 + 文本语言模型，也可能预训练）
  - 匹配语音序列的 **token rate** 与文本序列的**token rate**（如果需要的话）

- 具体实现方式：语音编码器的输出序列长度较短（经过时间）时是一个 **linear layer**；较长时需要在这里压缩

  - **线性变换** / 词汇表扩展：对语音表示\($H^{sp}$\)进行线性变换，可理解为 “词汇表扩展”，语音token会被添加到原有的词汇表中

  - 有 strides 的 **CNN** ：保留时间信息的同时缩短序列长度

  - 基于连接主义时序分类（CTC）的压缩：为每个时间步分配一个在一组标签令牌上的**概率分布**，非空白概率高的时间步，对应可能包含重要语言信息的片段，然后合并重复的非空白标签（？）

  - **Q-Former**：把任意长度的语音特征序列，“整理” 成固定长度的向量，用**attention**的方式映射到M个向量（M是超参数，对应不同方面的特征，这M个特征向量是训练中得到的）

    <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119163418025.png" alt="image-20251119163418025" style="zoom:50%;" /> 

    由于用了attention，丢失了时间顺序，可以用窗口级 Q-Former解决：

    1. 把长语音特征序列 `X` 切成多个 “小窗口”（比如每 20 个 $x_i $算一个窗口）；
    2. 对每个小窗口单独用 Q-Former 处理，得到 M 个向量；
    3. 最后把所有窗口的输出按顺序拼起来

  - AlignFormer：CTC 压缩＋Q-Former

### 3.3 Sequence Model

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119184836758.png" alt="image-20251119184836758" style="zoom:50%;" /> 

开始生成 token ~

#### 3.3.1 token生成

与文本令牌不同，音频令牌可能包含**粗粒度token与细粒度token的组合** （例如，将语音学token作为粗粒度token，将音频编解码器token作为细粒度token）

具体架构如下：

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119170923481.png" alt="image-20251119170923481" style="zoom:50%;" /> 

- **先粗后细**（图a）：首先生成所有时间步的粗粒度令牌，接着以粗粒度令牌为条件生成细粒度令牌，最后生成更细粒度的令牌（自回归）。（如AudioLM）
- **粗细令牌交替**（图b）：粗粒度、细粒度和更细粒度这三类令牌沿**时间轴对齐**并交织生成。在预测时间步t的令牌时，与t对应的粗粒度、细粒度和更细粒度令牌会被依次生成
- **时序生成 + 深度生成**（图c）：大Transformer （蓝色）对时间轴上的**帧间信息**进行建模；小 Transformer 头（绿色）捕捉**帧内关联**，在同一时间步内预测**多层令牌**
- **Delay pattern**（图d）：Transformer的方法下粗粒度令牌和细粒度令牌之间存在时间延迟，生成细粒度令牌时可以将**未来**的粗粒度令牌作为条件

上述方法存在策略复杂，延迟大的问题，不适用于实时应用场景，因此要减少令牌化层数（如将语音学token和音频编解码器token的生成整合到一个令牌器）

#### 3.3.2 文本与音频混合生成

主要方法：将**文本token**用作第一层token，随后再生成语音token（使用了文本模型，质量和真实性up）

文本token与语音token：语音具有固定采样率，而文本无固定采样率。文本token序列通常也比对应的语音token序列更短。混合解码需解决**长度不一致**的问题，且理想情况下还需实现**时序对齐**。

具体方法：

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119181355146.png" alt="image-20251119181355146" style="zoom:50%;" /> 

- 在文本 token 序列后添加 **padding token**，解决序列长度不匹配的问题（文本序列会先结束，随后生成的文本将引导语音token序列的生成，类似文本转语音，图a）
- 在**每个**文本token后添加固定的 **padding token**，同时也在语音token之间添加填充令牌，使两个序列长度一致（先生成文本token，利用生成的文本token预测语音学token，图b）
- **动态调整**文本token之间的padding token数量，生成模型会学习插入padding token，使文本序列与语音序列长度相同（在训练过程中，会使用**时序对齐**的文本 - 语音配对数据来构建文本序列）
- 语音令牌和文本令牌**交替在单个序列**中，文本 - 语音令牌的对齐关系是提前确定的

### 3.4 Speech Decoder

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119184919079.png" alt="image-20251119184919079" style="zoom:50%;" /> 

将语音表示 （连续特征 / 语音学token / 音频编解码器token） 转换回**波形**

具体方法：

- 用于处理**连续特征**的声码器（**Vocoder**）
- 基于 HiFi-GAN，适用于**语音学token**的基于单元的声码器（**Unit-based vocoder**）
- 编解码器解码器（**Codec decoder**）

## 4 模型训练

预训练与后训练：

- 预训练：文本预训练与**后训练**、标签语音语料库训练
- 后训练：下游任务训练

### 4.1 Pre-training strategies

介绍语音训练部分

#### 4.1.1 Generative pre-training

- **纯语音**预训练 **p(speech)**：利用无标签语音训练一个语音概率模型

  - 通常将语音转为token，再用经过训练以执行 “下一个语音token预测” 任务
  - 纯语音语言模型通常以**离散**的语音学token作为基本单元，但也可融入**时长、音高**之类的非语音学信息

- **语音 - 文本**联合预训练 **p(text, speech)**：利用**对齐的**语音与文本对口语语言模型进行预训练

  - 语音序列与文本序列可以交替，也可以为双通道

- 基于文本预训练的**持续预训练 ** **p(text)**：在额外的特定领域或特定模态数据上，对已完成预训练的模型进行进一步训练的过程

  类似把预训练好的文本模型再做语音数据的预训练 → 生成一个**同时具备文本和语音生成能力**的预训练模型

#### 4.1.2 Conditional pre-training p(text|speech)

把语音转文本、文本转语音模型当做预训练模型，可在大量带**转录文本的语音数据**上进行训练，因此已经掌握了语音与文本模态的**对齐**能力

#### 4.1.3 Aligning speech and text modalities

与任务无关的方式**对齐**语音与文本模态

- **隐式对齐**（Implicit Alignment）：用 ASR 数据集里 “语音 - 文本对”，让文本 LLM 给文字版输入出一个 “标准答案”，再让 SLM 听对应的语音版输入，训练 SLM 也输出这个 “标准答案”（不用每个字和声音完全匹配）
- **显式对齐**（Explicit Alignment）：通过合适的距离 / **相似度度量**，将语音特征与对应的文本嵌入进行匹配（逐字对齐）

### 4.2 Post-training strategies

#### 4.2.1 Task-specific training

采用含**任务指定符**的多任务训练方式：$p(·|speech, ⟨taskspecifier⟩)$

#### 4.2.2 Instruction tuning $(p(·|speech, instruction))$

指令微调数据通常包含一段**语音**录音、一条描述语音任务的**指令**（语音和文本都可以），以及对应的**真实输出**

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251119194706103.png" alt="image-20251119194706103" style="zoom:50%;" /> 

**数据**生成方式：有音频数据下可以转录为文本；有文本数据下转成语音

- **task转换为instruction**：将任务特定标签替换为**自然语言**指令
-  **基于语音的问答（SQA）数据**：借助 ChatGPT 等语言模型生成，将语音录音的**转录文本**作为输入提供给语言模型，生成对应的问答对（可以融入音频中的细粒度信息作为描述）
- **文本指令微调数据**的合成：用 TTS 把文字转成语音
- **组合式指令**：将单个任务的指令组合形成更复杂的指令

#### 4.2.3 Chat SLM training

主要涉及两个方向：

- 基于具备聊天能力的文本语言模型，构建具有**语音感知能力**（就是理解和生成有关语音的描述）的文本语言模型（利用性能更强的文本语言模型，生成以语音录音为核心的**文本对话**，并使用这类 “伪对话数据” 对 SLM 进行微调）
- 构建纯语音语言模型或语音 + 文本语言模型，以处理 “语音输入 - 语音输出” 的对话。（用**TTS**从文本数据集中生成语音对话数据，提供真实对话中存在的**更自然、更具自发性**的表现，但通常**噪声更多**，需经过精心预处理）

#### 4.3 Other training details

PEFT等微调方式，与LLM类似

## 5 当前的SLM


| 模型类别                | 训练数据                                                                 | 关键能力                                                                 | 代表模型                                                                 |
|:-----------------------:|:----------------------------------------------------------------------------:|:------------------------------------------------------------------------:|:------------------------------------------------------------------------:|
| 纯语音                 （Pure Speech LM） | 以**无标签语音数据**为核心，训练单位为离散语音token | 实现语音续写、生成连贯语音片段，部分模型支持韵律建模（如音高、时长控制）   | GSLM、pGSLM、AudioLM、TWIST、SpeechSSM、Align-SLM                        |
| 语音+文本LM（Speech+Text LM） | 依赖对齐的**语音-文本**数据，可通过TTS从文本数据集合成语音对话数据，或复用现有文本对话数据集 | 理解并生成**双模态**内容，支持长时长独白、多轮对话，部分模型具备**实时推理**能力 | Moshi、SpiRit-LM、SpeechGPT、**Mini-Omni**、**LLaMA-Omni**、GLM-4-Voice  |
| 语音感知文本LM（Speech-aware text LM） |以文本输出为主要形式 | 需语音数据与文本指令数据结合，继承文本LLM的预训练基础，额外补充语音-文本模态对齐训练 | WavPrompt、SALMONN、**Qwen-Audio**、SLM、DeSTA、LTU-AS、DiscreteSLU、UniverSLU、WavLLM、BESTOW |